{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCuS6Q1G-GZp"
   },
   "source": [
    "\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/akshayrb22/playing-with-data/blob/master/supervised_learning/support_vector_machine/svm.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfNkgA6l-GZt"
   },
   "source": [
    "# Support Vector Machine Classification\n",
    "\n",
    "\n",
    "\n",
    "## What are some use cases for SVMs?\n",
    "\n",
    "- Classification \n",
    "- Regression (time series prediction, etc) \n",
    "- Outlier detection \n",
    "- Clustering\n",
    "\n",
    "\n",
    "## How does an SVM compare to other ML algorithms?\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://image.slidesharecdn.com/mscpresentation-140722065852-phpapp01/95/msc-presentation-bioinformatics-7-638.jpg?cb=1406012610\"/>\n",
    "</p>\n",
    "\n",
    "- As a rule of thumb, SVMs are great for relatively small data sets with fewer outliers. \n",
    "- Other algorithms (Random forests, deep neural networks, etc.) require more data but almost always come up with very robust models.\n",
    "- The decision of which classifier to use depends on your dataset and the general complexity of the problem.\n",
    "- \"Premature optimization is the root of all evil (or at least most of it) in programming.\" - Donald Knuth, CS Professor (Turing award speech 1974)  \n",
    "\n",
    "\n",
    "## What is a Support Vector Machine?\n",
    "\n",
    "It's a supervised machine learning algorithm which can be used for both classification or regression problems. But it's usually used for classification. Given 2 or more labeled classes of data, it acts as a discriminative classifier, formally defined by an optimal hyperplane that seperates all the classes. New examples that are then mapped into that same space can then be categorized based on on which side of the gap they fall.\n",
    "\n",
    "## What are Support Vectors?\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://www.dtreg.com/uploaded/pageimg/SvmMargin2.jpg\"/>\n",
    "</p>\n",
    " \n",
    "Support vectors are the data points nearest to the hyperplane, the points of a data set that, if removed, would alter the position of the dividing hyperplane. Because of this, they can be considered the critical elements of a data set, they are what help us build our SVM. \n",
    "\n",
    "## What is a hyperplane?\n",
    "<p align=\"center\">\n",
    "  <img src=\"http://slideplayer.com/slide/1579281/5/images/32/Hyperplanes+as+decision+surfaces.jpg\"/>\n",
    "</p>\n",
    "\n",
    "Geometry tells us that a hyperplane is a subspace of one dimension less than its ambient space. For instance, a hyperplane of an n-dimensional space is a flat subset with dimension n − 1. By its nature, it separates the space into two half spaces.\n",
    "\n",
    "## Let's define our loss function (what to minimize) and our objective function (what to optimize)\n",
    "\n",
    "#### Loss function\n",
    "\n",
    "We'll use the Hinge loss. This is a loss function used for training classifiers. The hinge loss is used for **\"maximum-margin\"** classification, most notably for support vector machines (SVMs).\n",
    "<p align=\"center\">\n",
    "  <img src=\"http://i.imgur.com/OzCwzyN.png\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "c is the loss function, x the sample, y is the true label, f(x) the predicted label.\n",
    "<p align=\"center\">\n",
    "  <img src=\"http://i.imgur.com/FZ7JcG3.png\"/>\n",
    "</p>\n",
    " \n",
    "#### Objective Function\n",
    "<p align=\"center\">\n",
    "  <img src=\"http://i.imgur.com/I5NNu44.png\"/>\n",
    "</p>\n",
    "\n",
    "As you can see, our objective of a SVM consists of two terms. The first term is a regularizer, the heart of the SVM, the second term the loss. The regularizer balances between margin maximization and loss. We want to find the decision surface that is maximally far away from any data points.\n",
    "\n",
    "**How do we minimize our loss/optimize for our objective (i.e learn)?**\n",
    "\n",
    "We have to derive our objective function to get the gradients! Gradient descent ftw.  As we have two terms, we will derive them seperately using the sum rule in differentiation.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"http://i.imgur.com/6uK3BnH.png\"/>\n",
    "</p>\n",
    "\n",
    "This means, if we have a misclassified sample, we update the weight vector w using the gradients of both terms, else if classified correctly,we just update w by the gradient of the regularizer.\n",
    "\n",
    "\n",
    "\n",
    "Misclassification condition \n",
    "<p align=\"center\">\n",
    "  <img src=\"http://i.imgur.com/g9QLAyn.png\"/>\n",
    "</p>\n",
    "\n",
    "Update rule for our weights (misclassified)\n",
    "<p align=\"center\">\n",
    "  <img src=\"http://i.imgur.com/rkdPpTZ.png\"/>\n",
    "</p>\n",
    "\n",
    "including the learning rate η and the regularizer λ\n",
    "The learning rate is the length of the steps the algorithm makes down the gradient on the error curve.\n",
    "- Learning rate too high? The algorithm might overshoot the optimal point.\n",
    "- Learning rate too low? Could take too long to converge. Or never converge.\n",
    "\n",
    "The regularizer controls the trade off between the achieving a low training error and a low testing error that is the ability to generalize your classifier to unseen data. As a regulizing parameter we choose 1/epochs, so this parameter will decrease, as the number of epochs increases.\n",
    "- Regularizer too high? overfit (large testing error) \n",
    "- Regularizer too low? underfit (large training error) \n",
    "\n",
    "Update rule for our weights (correctly classified)\n",
    "<p align=\"center\">\n",
    "  <img src=\"http://i.imgur.com/xTKbvZ6.png\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Trick\n",
    "SVM has a technique called the kernel trick. These are functions that take low dimensional input space and transform it into a higher-dimensional space i.e. it converts not separable problem to separable problem. It is mostly useful in non-linear separation problems. This is shown as follows:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://editor.analyticsvidhya.com/uploads/93804svm20.png\"/>\n",
    "</p>\n",
    "\n",
    "In a more precise manner, Nonlinear can be explained by another visual representation as below, where it can be clearly seen that how the data points which is not able to linearly classified is converted into higher dimensional, then get separated linearly in higher space, which is back non-linearly separated in the original dimension of lower space.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://editor.analyticsvidhya.com/uploads/70614svm22.png\"/>\n",
    "</p>\n",
    "\n",
    "There are some famous and most frequently used Non-linear kernels in SVM are,\n",
    "\n",
    "1. Polynomial SVM Kernel\n",
    "\n",
    "2. Gaussian Radial Basis Function (RBF)\n",
    "\n",
    "3. Sigmoid Kernel"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "svm.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
